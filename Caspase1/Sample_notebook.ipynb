{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistics - Optional - Scikit libraries can be called instead\n",
    "def accuracy(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    acc = float(\"{0:.2f}\".format(acc))\n",
    "    return acc\n",
    "\n",
    "def balanced_accuracy(y_true, y_pred):\n",
    "    bacc = balanced_accuracy_score(y_true, y_pred)\n",
    "    bacc = float(\"{0:.2f}\".format(bacc))\n",
    "    return bacc\n",
    "\n",
    "def sensitivity_specificity(y_true, y_pred):\n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "    TP = confusion[1, 1]\n",
    "    TN = confusion[0, 0]\n",
    "    FP = confusion[0, 1]\n",
    "    FN = confusion[1, 0]\n",
    "    accuracy = (TP+TN)/float(TP+TN+FP+FN)\n",
    "    accuracy = float(\"{0:.2f}\".format(accuracy))\n",
    "    sensitivity = TP / float(FN + TP)\n",
    "    sensitivity = float(\"{0:.2f}\".format(sensitivity))\n",
    "    specificity = TN / float(TN + FP)\n",
    "    specificity = float(\"{0:.2f}\".format(specificity))\n",
    "    return sensitivity, specificity\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "    TP = confusion[1, 1]\n",
    "    TN = confusion[0, 0]\n",
    "    FP = confusion[0, 1]\n",
    "    FN = confusion[1, 0]\n",
    "    f1_score = TP /float(TP +((FP+FN)/2.))\n",
    "    f1_score = float(\"{0:.2f}\".format(f1_score))\n",
    "    return f1_score\n",
    "    \n",
    "def auc_roc(y_true, y_pred_prob):\n",
    "    auc = roc_auc_score(y_true, y_pred_prob)\n",
    "    auc = float(\"{0:.2f}\".format(auc))\n",
    "    return auc\n",
    "\n",
    "def kappa_score(y_true, y_pred):\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    kappa = float(\"{0:.2f}\".format(kappa))\n",
    "    return kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and Segement Data\n",
    "#Importing Data\n",
    "dataset = pd.read_csv('Caspase_data.csv')  #Change Data File Name\n",
    "#dataset.drop(['Name'], axis=1)\n",
    "X = dataset.iloc[:,1:39].values   #Seggregated Descriptor sets\n",
    "y = dataset.iloc[:,0:1]\n",
    "y = np.array(y).ravel()\n",
    "\n",
    "\n",
    "\n",
    "#Splitting in to Training and Test sets\n",
    "np.random.seed(43)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)\n",
    "\n",
    "#If you are interested in storing data sets\n",
    "#pd.DataFrame(X_train).to_csv(\"train_x.csv\", header=None, index=None)\n",
    "#pd.DataFrame(X_test).to_csv(\"test_x.csv\", header=None, index=None)\n",
    "#pd.DataFrame(y_train).to_csv(\"train_y.csv\", header=None, index=None)\n",
    "#pd.DataFrame(y_test).to_csv(\"test_y.csv\", header=None, index=None)\n",
    "#print(X)\n",
    "\n",
    "#Loading validation data\n",
    "dataset3 = pd.read_csv('validation.csv', delimiter=',')\n",
    "Validate = dataset3.iloc[:,1:39]\n",
    "print(\"loaded validation data: %s, %s\" % (Validate.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "# fit model on training data\n",
    "#Try different ones too\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "model3 = RandomForestClassifier(n_estimators = 300, random_state = 42)\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "\n",
    "#import pickle\n",
    "# save the model file\n",
    "#output = open('randomforest_classifier.pkl', 'wb')\n",
    "#pickle.dump(model, output)\n",
    "#output.close()\n",
    "\n",
    "\n",
    "# get predictions for test data\n",
    "y_pred = model3.predict(X_test)\n",
    "y_pred_prob = model3.predict_proba(X_test).T[1]\n",
    "y_pred_prob = y_pred_prob.ravel()\n",
    "y_pred_prob = np.round(y_pred_prob, 2)\n",
    "\n",
    "\n",
    "#get Validation data\n",
    "# get predictions for test data\n",
    "y_valid = model3.predict(Validate)\n",
    "print(y_valid)\n",
    "y_valid_prob = model3.predict_proba(Validate).T[1]\n",
    "y_valid_prob = y_valid_prob.ravel()\n",
    "y_valid_prob = np.round(y_valid_prob, 2)\n",
    "print(y_valid_prob)\n",
    "# calculate performance metrics\n",
    "\n",
    "auc = auc_roc(y_test, y_pred_prob)\n",
    "ba = balanced_accuracy(y_test, y_pred)\n",
    "sens, spec = sensitivity_specificity(y_test, y_pred)\n",
    "kappa = kappa_score(y_test, y_pred)\n",
    "accuracy_model = accuracy(y_test, y_pred)\n",
    "f1_score_model = f1_score(y_test, y_pred)\n",
    "\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "rf_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('RF: ROC AUC=%.3f' % (rf_auc))\n",
    "\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "rf_fpr, rf_tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(rf_fpr, rf_tpr, marker='.', label='Logistic')\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "#Precision Recall Plot\n",
    "rf_precision, rf_recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "# plot the precision-recall curves\n",
    "no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "pyplot.plot(rf_recall, rf_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "#####################################################\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(confusion)\n",
    "print('model performance')\n",
    "print('AUC:\\t%s' % auc)\n",
    "print('BACC:\\t%s' % ba)\n",
    "print('Accuracy:\\t%s' % accuracy_model)\n",
    "print('F1-Score:\\t%s' % f1_score_model)\n",
    "print('Sensitivity:\\t%s' % sens)\n",
    "print('Specificity:\\t%s' % spec)\n",
    "print('Kappa:\\t%s' % kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "model2 = XGBClassifier(n_estimators = 300, random_state = 42, objective='binary:logistic', learning_rate=0.05)\n",
    "model2.fit(X_train, y_train)\n",
    "# save the model file\n",
    "#output = open('XGB_classifier.pkl', 'wb')\n",
    "#pickle.dump(model2, output)\n",
    "#output.close()\n",
    "\n",
    "\n",
    "# get predictions for test data\n",
    "y_pred = model2.predict(X_test)\n",
    "#y_pred_prob = model.predict_proba(X_test).T[1]\n",
    "#y_pred_prob = y_pred_prob.ravel()\n",
    "#y_pred_prob = np.round(y_pred_prob, 2)\n",
    "\n",
    "# calculate performance metrics\n",
    "\n",
    "#auc = auc_roc(y_test, y_pred_prob)\n",
    "#ba = balanced_accuracy(y_test, y_pred)\n",
    "#sens, spec = sensitivity_specificity(y_test, y_pred)\n",
    "#kappa = kappa_score(y_test, y_pred)\n",
    "#accuracy_model = accuracy(y_test, y_pred)\n",
    "#f1_score_model = f1_score(y_test, y_pred)\n",
    "#####################################################\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(confusion)\n",
    "\n",
    "# get predictions for test data\n",
    "y_valid = model3.predict(Validate)\n",
    "print(y_valid)\n",
    "\n",
    "sens, spec = sensitivity_specificity(y_test, y_pred)\n",
    "kappa = kappa_score(y_test, y_pred)\n",
    "accuracy_model = accuracy(y_test, y_pred)\n",
    "f1_score_model = f1_score(y_test, y_pred)\n",
    "\n",
    "print('Accuracy:\\t%s' % accuracy_model)\n",
    "print('F1-Score:\\t%s' % f1_score_model)\n",
    "print('Sensitivity:\\t%s' % sens)\n",
    "print('Specificity:\\t%s' % spec)\n",
    "print('Kappa:\\t%s' % kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vectors\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "%matplotlib inline\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 12\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)\n",
    "plt.rcParams['figure.dpi']=150\n",
    "\n",
    "\n",
    "svc = SVC()\n",
    "training_start = time.perf_counter()\n",
    "svc.fit(X_train, y_train)\n",
    "training_end = time.perf_counter()\n",
    "prediction_start = time.perf_counter()\n",
    "preds = svc.predict(X_test)\n",
    "prediction_end = time.perf_counter()\n",
    "acc_svc = (preds == y_test).sum().astype(float) / len(preds)*100\n",
    "svc_train_time = training_end-training_start\n",
    "svc_prediction_time = prediction_end-prediction_start\n",
    "print(\"Scikit-Learn's Support Vector Machine Classifier's prediction accuracy is: %3.2f\" % (acc_svc))\n",
    "print(\"Time consumed for training: %4.3f seconds\" % (svc_train_time))\n",
    "print(\"Time consumed for prediction: %6.5f seconds\" % (svc_prediction_time))\n",
    "\n",
    "\n",
    "print('model performance')\n",
    "print('AUC:\\t%s' % auc)\n",
    "print('BACC:\\t%s' % ba)\n",
    "print('Accuracy:\\t%s' % accuracy_model)\n",
    "print('F1-Score:\\t%s' % f1_score_model)\n",
    "print('Sensitivity:\\t%s' % sens)\n",
    "print('Specificity:\\t%s' % spec)\n",
    "print('Kappa:\\t%s' % kappa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search with DNN\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "np.random.seed(43)\n",
    "\n",
    "monitor = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=100,\n",
    "    min_delta=0.001,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "def create_model(dropout_rate=0.0, neurons=32, layers=1):\n",
    "\n",
    "    model = Sequential()\n",
    "        \n",
    "    while layers > 0:\n",
    "\n",
    "        layers -= 1\n",
    "\n",
    "        model.add(Dense(neurons, kernel_initializer='he_uniform', activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1, kernel_initializer='he_uniform', activation='linear'))\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer='adam')\n",
    " \n",
    "    return model\n",
    "\n",
    "X_test = StandardScaler().fit_transform(X_test)\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "\n",
    "model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3]\n",
    "neurons = [64, 128, 256, 512]\n",
    "layers = [8, 16, 32]\n",
    "\n",
    "param_grid = dict(dropout_rate=dropout_rate, neurons=neurons, layers=layers)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', n_jobs=-1, cv=5, verbose=2)\n",
    "grid_result = grid.fit(X_train, y_train, callbacks=[monitor], batch_size=32, epochs=10000)\n",
    "\n",
    "'''\n",
    "# save the model file\n",
    "output = open('dnn_2_classifier.pkl', 'wb')\n",
    "pickle.dump(grid, output)\n",
    "output.close()\n",
    "'''\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "    \n",
    "# get predictions for test data\n",
    "predictions = grid.predict(X_test)\n",
    "y_pred = np.round(predictions, 0)\n",
    "predictions = np.array(predictions).ravel()\n",
    "predictions = np.round(predictions, 2)\n",
    "\n",
    "\n",
    "# calculate performance metrics\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(confusion)\n",
    "auc = auc_roc(y_test, predictions)\n",
    "ba = balanced_accuracy(y_test, y_pred)\n",
    "sens, spec = sensitivity_specificity(y_test, y_pred)\n",
    "kappa = kappa_score(y_test, y_pred)\n",
    "accuracy_model = accuracy(y_test, y_pred)\n",
    "f1_score_model = f1_score(y_test, y_pred)\n",
    "#########################################################\n",
    "print('model performance')\n",
    "print('AUC:\\t%s' % auc)\n",
    "print('BACC:\\t%s' % ba)\n",
    "print('Accuracy:\\t%s' % accuracy_model)\n",
    "print('F1-Score:\\t%s' % f1_score_model)\n",
    "print('Sensitivity:\\t%s' % sens)\n",
    "print('Specificity:\\t%s' % spec)\n",
    "print('Kappa:\\t%s' % kappa)\n",
    "\n",
    "#Validation\n",
    "y_valid = grid.predict(Validate)\n",
    "print(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
